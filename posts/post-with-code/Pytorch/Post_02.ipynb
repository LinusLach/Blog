{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Gentle (Mathematicians) Introduction to PyTorch and Neural Networks Part 02\"\n",
    "author: \"Linus Lach\"\n",
    "date: \"2023-07-05\"\n",
    "categories: [Python, PyTorch, Machine Learning, Gradient Descent, Softmax]\n",
    "image: \"image.png\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python3\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle (Mathematicians) Introduction to PyTorch and Neural Networks Part 02\n",
    "## Gradient descent in one dimension\n",
    "In this blog post, I'd like to introduce a common method used for training machine learning models such as the  [logistic model](https://linus-lach.de/posts/post-with-code/pytorch/post_01).\n",
    " This approach is commonly known as gradient descentâ€”a method that, as its name implies, involves minimizing a function by progressively descending along its gradient.\n",
    "  In the context of regression, typically a loss function such as the mean squared error is minimized, which in turn yields an optimal fit for a given model.\n",
    "\n",
    "Mathematically speaking in its most basic form this translates into the following:\\\n",
    "Let $\\Omega\\subseteq \\mathbb{R}$ and consider a function $f:\\Omega \\to \\mathbb{R}$ that is at least one time differentiable in $\\Omega$. Set an initial value $x_0\\in\\Omega$, a step size $\\alpha \\geq 0$ and iterate for $n = 0,...,N$ through the following steps:\\\n",
    "&nbsp; &nbsp; 1. Calculate  $d_n = -f'(x_n)$,\\\n",
    "&nbsp; &nbsp; 2. Set $x_{n+1} = x_{n} + \\alpha d.$\n",
    "After iterating through all steps, return the last value $x_N$.\n",
    "\n",
    "The procedure above ensures that $f(x_0) \\geq f(x_1) \\geq ... \\geq f(x_N)$ for a sufficiently small step size $\\alpha$, since each $x_n$ moves along the negative gradient towards a local minimum.\n",
    "\n",
    "## A first example\n",
    "Let $f:\\mathbb{R}\\to\\mathbb{R}, \\: x\\mapsto (x-2)^2$ and set $x_0 = -1,\\,\\alpha = 0.1, N = 20$. Then, the following interactive plot visualizes each step of the gradient descent towards the minimum at $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T15:58:28.061176Z",
     "end_time": "2023-06-02T15:58:28.087744Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some packages needed throughout the article\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T15:32:04.059528Z",
     "end_time": "2023-06-02T15:32:04.114226Z"
    }
   },
   "outputs": [],
   "source": [
    "N=200\n",
    "X = np.linspace(-2,6,N)\n",
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def gd_1d(epochs ,lr ,f ,x):\n",
    "    coord = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = f(x)\n",
    "        coord.append([x.data,loss.data])\n",
    "        loss.backward()\n",
    "        x.data = x.data - lr * x.grad.data\n",
    "        x.grad.data.zero_()\n",
    "\n",
    "    return np.transpose(np.reshape(coord,(epochs,2)))\n",
    "x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "lr = 0.1\n",
    "epochs = 20\n",
    "coord = gd_1d(epochs,lr,f,x0)\n",
    "\n",
    "#Uncomment for a static version\n",
    "#plt.plot(X,f(X))\n",
    "#plt.plot(coord[0],coord[1],'-or')\n",
    "#plt.xlim((-2,6))\n",
    "#plt.ylim((-2,10))\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<iframe src='gd_1d_xsq.html' width=700 height=450></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the step size increases, the $x_n$ slowly approaches the minimum at $x=0$.\\\n",
    " There are however, quite a few pitfalls when applying gradient descent.\n",
    " Consider the function\n",
    " \\begin{equation*}\n",
    " x\\mapsto \\frac{1}{2}*(\\frac{3}{4}*x-1.2)^4-2*(\\frac{3}{4}*x-1)^2+2\n",
    " \\end{equation*}\n",
    " which has a gloabl minimum at $x \\approx 3.607$ and is displayed below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "<iframe src='gd_1d_xquartic.html' width=700 height=450></iframe>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As in the example before, set $x_0 = -1,\\,\\alpha = 0.1, N = 20$ which results in the following interactive plot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T15:32:07.990392Z",
     "end_time": "2023-06-02T15:32:08.051639Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5*(0.75*x-1.2)**4-2*(0.75*x-1)**2+2\n",
    "\n",
    "x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "lr = 0.1\n",
    "epochs = 20\n",
    "coord = gd_1d(epochs,lr,f,x0)\n",
    "#Uncomment for a static version\n",
    "#plt.plot(X,f(X))\n",
    "#plt.plot(coord[0],coord[1],'-or')\n",
    "#plt.xlim((-2,6))\n",
    "#plt.ylim((-2,10))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<iframe src='gd_1d_xquartic_lr01.html' width=700 height=450></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It turns out that the step size is too small, so instead of approaching the global minimum at $x \\approx 3.6$, the algorithm is stuck in the local minimum at $x\\approx -0.1$.\n",
    "This can easily fixed by increasing the step size to $0.4$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T15:32:09.071643Z",
     "end_time": "2023-06-02T15:32:09.164268Z"
    }
   },
   "outputs": [],
   "source": [
    "x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "lr = 0.40\n",
    "epochs = 20\n",
    "coord = gd_1d(epochs,lr,f,x0)\n",
    "\n",
    "#Uncomment for a static version\n",
    "#plt.plot(X,f(X))\n",
    "#plt.plot(coord[0],coord[1],'-or')\n",
    "#plt.xlim((-2,6))\n",
    "#plt.ylim((-2,10))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<iframe src='gd_1d_xquartic_lr04.html' width=700 height=450></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using a step size of $0.4$ already improves the result.\n",
    " However, for $n\\geq 11$ the $x_n$ are no longer approaching the global minimum rather than jumping around it from one side to the other.\n",
    "\n",
    "## Improving the step size\n",
    "So how can an optimal step size be found? The initial change to a step size of $0.4$ seems quite arbitrary!\n",
    "One way is running a grid search, where gradient descent is performed multiple times with varying step sizes.\n",
    "After iterating through every step size, select the one which yields the best result.\n",
    "For example, we could start with a step size of $\\alpha =0.1$ and work our way up to $1.0$ with increments of $0.01$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "res = np.array([])\n",
    "for lr in np.arange(0.1,1,0.01):\n",
    "    x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "    epochs = 20\n",
    "    coord = gd_1d(epochs,lr,f,x0)\n",
    "    res = np.append(res,np.min(coord[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This approach raises some new challenges, as certain step sizes cause the gradient descent to diverge. Set for example the step size to $\\alpha = 0.79$ and check out what happens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "lr = 0.79\n",
    "epochs = 20\n",
    "coord = gd_1d(epochs,lr,f,x0)\n",
    "\n",
    "#Uncomment for a static version\n",
    "#plt.plot(X,f(X))\n",
    "#plt.plot(coord[0],coord[1],'-or')\n",
    "#plt.xlim((-2,6))\n",
    "#plt.ylim((-2,10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "<iframe src='gd_1d_xquartic_lr079.html' width=700 height=450></iframe>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By limiting ourselves to the results where $x_N$ is finite we obtain the following result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a learning rate of 0.52, the final value for x_N is 3.613\n"
     ]
    }
   ],
   "source": [
    "opt_lr = np.arange(0.1,1,0.01)[np.argmin(res[~np.isnan(res)])+1]\n",
    "x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "print(\"For a step size of {lr:.2f}, the final value for x_N is {res:.3f}\".format(lr = opt_lr,res = gd_1d(epochs,opt_lr,f,x0)[0][-1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "<iframe src='gd_1d_xquartic_lr052.html' width=700 height=450></iframe>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the final value $x_N$ is already close to the global minimum at $x \\approx 3.607$, this approach is not stable at all.\n",
    "Setting $N = 19$, yields a final result of $x_N \\approx 2.491$, which misses the global minimum by around $1.116$. This phenomenon motivates the last part of this post.\n",
    "\n",
    "## Variable step sizes\n",
    "\n",
    "In this last paragraph we discuss the so-called *Armijo--Rule*.\n",
    " It is a step size selection strategy designed to ensure that in each iteration of the gradient descent the step size $\\alpha$ is sufficiently large to make progress, and additionally to ensure that a significant decrease in the objective function value is achieved at the same time.\n",
    "  By following this rule, we can address all the previous issues at once!\n",
    "The basic idea for finding this optimal step size $\\alpha$ in each of the $n=1,...,N$ steps of the gradient descent is outlined below.\\\n",
    "Set $\\beta,\\gamma\\in(0,1)$ and define an iteration limit $M$ for the following procedure:\n",
    "\n",
    "For each $j = 1,...,M$\n",
    "&nbsp; &nbsp; 1. Check if $f(x_n + \\alpha  d_n) \\leq f(x_n) - \\gamma  \\alpha  d_n^2$.\\\n",
    "&nbsp; &nbsp; 2. If the condition above is not fulfilled, set $\\alpha = \\alpha\\beta$.\\\n",
    "&nbsp; &nbsp; 3. Repeat until 1. is fulfilled or until $j=M$ and perform th next step of gradient descent with the updated $\\alpha$.\n",
    "By iteratively decreasing the step size $\\alpha$, we ensure that the bigger jumps observed before do not occur in a neighbourhood of the global minimum."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "def gd_1d_arm(epochs ,lr ,f ,x, gamma, beta, max_iter):\n",
    "    coord = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = f(x)\n",
    "        coord.append([x.data,loss.data])\n",
    "        loss.backward()\n",
    "        for i in range(max_iter):\n",
    "            if f(x - lr * x.grad.data) <= f(x) - gamma * lr * x.grad.data**2:\n",
    "                break\n",
    "            lr *= beta\n",
    "        x.data = x.data - lr * x.grad.data\n",
    "        x.grad.data.zero_()\n",
    "    return np.transpose(np.reshape(coord,(epochs,2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [],
   "source": [
    "x0 = torch.tensor(-1.0,requires_grad=True)\n",
    "lr = 0.6\n",
    "gamma = 0.9\n",
    "beta=0.98\n",
    "epochs = 20\n",
    "coord = gd_1d_arm(epochs,lr,f,x0,gamma,beta,max_iter=10)\n",
    "\n",
    "#Uncomment for a static version\n",
    "#plt.plot(X,f(X))\n",
    "#plt.plot(coord[0],coord[1],'-or')\n",
    "#plt.xlim((-2,6))\n",
    "#plt.ylim((-2,10))\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "<iframe src='gd_1d_xquartic_armijo.html' width=700 height=450></iframe>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
